{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UU_LChJBaQc0",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "file_path = \"./drive/MyDrive/Colab Notebooks/base_turkish_misspellings.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "wrong_words = data[\"wrong\"].values\n",
    "correct_words = data[\"correct\"].values\n",
    "\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "all_chars = set(\"\".join(wrong_words) + \"\".join(correct_words)) | {EOS_TOKEN}\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(all_chars))}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "input_dim = len(char_to_idx) + 1\n",
    "hidden_dim = 100\n",
    "output_dim = len(char_to_idx) + 1\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "max_length = max(max(len(word) for word in wrong_words), max(len(word) for word in correct_words)) + 1\n",
    "\n",
    "def encode_word(word, char_to_idx, max_length):\n",
    "    encoded = [char_to_idx[char] for char in word] + [char_to_idx[EOS_TOKEN]]\n",
    "    return encoded + [0] * (max_length - len(encoded))\n",
    "\n",
    "def decode_sequence(sequence, idx_to_char):\n",
    "    result = []\n",
    "    for idx in sequence:\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        char = idx_to_char[idx]\n",
    "        if char == EOS_TOKEN:\n",
    "            break\n",
    "        result.append(char)\n",
    "    return \"\".join(result)\n",
    "\n",
    "class SpellCorrectorDataset(Dataset):\n",
    "    def __init__(self, wrong_words, correct_words):\n",
    "        self.wrong_words = torch.LongTensor([encode_word(w, char_to_idx, max_length) for w in wrong_words])\n",
    "        self.correct_words = torch.LongTensor([encode_word(c, char_to_idx, max_length) for c in correct_words])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wrong_words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.wrong_words[idx], self.correct_words[idx]\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_output, decoder_output):\n",
    "        score = self.V(torch.tanh(self.W1(encoder_output) + self.W2(decoder_output)))\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        context_vector = attention_weights * encoder_output\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class SpellCorrectorModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SpellCorrectorModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim, padding_idx=0)\n",
    "        self.bilstm1 = nn.LSTM(hidden_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.bilstm2 = nn.LSTM(hidden_dim * 2, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.attention = AttentionLayer(hidden_dim * 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim * 4, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        encoder_output, _ = self.bilstm1(x)\n",
    "        encoder_output = self.dropout(encoder_output)\n",
    "        encoder_output, _ = self.bilstm2(encoder_output)\n",
    "        encoder_output = self.dropout(encoder_output)\n",
    "\n",
    "        context_vector, attention_weights = self.attention(encoder_output, encoder_output)\n",
    "        context_vector = context_vector.unsqueeze(1)\n",
    "        context_vector = context_vector.repeat(1, encoder_output.size(1), 1)\n",
    "\n",
    "        decoder_input = torch.cat([encoder_output, context_vector], dim=-1)\n",
    "        output = self.fc(decoder_input)\n",
    "        return torch.log_softmax(output, dim=-1)\n",
    "\n",
    "model = SpellCorrectorModel(input_dim, hidden_dim, output_dim).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset = SpellCorrectorDataset(wrong_words, correct_words)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (wrong, correct) in enumerate(train_loader):\n",
    "            wrong, correct = wrong.to(device), correct.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(wrong)\n",
    "\n",
    "            loss = criterion(output.view(-1, output_dim), correct.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch: {epoch+1}, Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "def correct_word(word):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded = torch.LongTensor([encode_word(word, char_to_idx, max_length)]).to(device)\n",
    "        output = model(encoded)\n",
    "        predicted_indices = torch.argmax(output, dim=-1).cpu().numpy().squeeze()\n",
    "        return decode_sequence(predicted_indices, idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgfGJBWkaQc2",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "parameters = {\n",
    "    \"char_to_idx\": char_to_idx,\n",
    "    \"idx_to_char\": idx_to_char,\n",
    "    \"max_length\": max_length,\n",
    "    \"eos_token\":EOS_TOKEN,\n",
    "    \"hidden_dim\":hidden_dim,\n",
    "}\n",
    "\n",
    "with open(\"params.json\", \"w\") as f:\n",
    "    json.dump(parameters, f,indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
